{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1def7355",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33m/content/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m debug = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m     19\u001b[39m drive.mount(path+\u001b[33m'\u001b[39m\u001b[33m/drive/\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug == \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "import skimage\n",
    "from skimage import __version__ as skimage_version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "\n",
    "path = \"/content/\"\n",
    "debug = True\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(path+'/drive/')\n",
    "\n",
    "if debug == True:\n",
    "    A5_savepath = path+'/drive/MyDrive/03 McGill EE Semester 5 (Fall 2025)/ECSE 415 (Clark)/A5/'\n",
    "\n",
    "print(f\"Python version:           {os.sys.version.split()[0]}\")\n",
    "print(f\"OpenCV version:           {cv2.__version__}\")\n",
    "print(f\"NumPy version:            {np.__version__}\")\n",
    "print(f\"Matplotlib version:       {matplotlib.__version__}\")\n",
    "print(f\"PyTorch version:          {torch.__version__}\")\n",
    "print(f\"scikit-image version:     {skimage_version}\")\n",
    "print(f\"scikit-learn version:     {sklearn_version}\")\n",
    "print(\"Path: \" + path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7cde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug == True:\n",
    "    os.chdir(path)\n",
    "    !pip install kaggle\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp /content/drive/MyDrive/Kaggle_API/kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    !kaggle competitions download -c ecse-415-video-analysis\n",
    "    !unzip -q ecse-415-video-analysis.zip -d ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010737a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PART 1: Data Preparation (10 points) =====\n",
    "# Convert Task1 images to video at 14 FPS and save as task1_input.mp4\n",
    "\n",
    "task1_dir = Path(path, \"Object_Tracking\", \"Task1\")\n",
    "csv1_path = Path(task1_dir, \"gt\", \"gt.txt\")\n",
    "ground_truth_task1 = np.loadtxt(csv1_path, delimiter=\",\")\n",
    "\n",
    "images1_path = Path(task1_dir, \"images\")\n",
    "\n",
    "# Get all image files and sort them by filename to ensure correct order\n",
    "image_files = sorted([f for f in os.listdir(images1_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "\n",
    "print(f\"Found {len(image_files)} images in {images1_path}\")\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(\"ERROR: No images found!\")\n",
    "else:\n",
    "    # Read first image to get dimensions\n",
    "    first_img_path = Path(images1_path, image_files[0])\n",
    "    first_frame = cv2.imread(str(first_img_path))\n",
    "    H, W, _ = first_frame.shape\n",
    "    \n",
    "    print(f\"Video dimensions: {W}x{H}\")\n",
    "    \n",
    "    # Set up VideoWriter with better codec compatibility\n",
    "    fps = 14.0  # Required FPS for Part 1\n",
    "    frame_size = (W, H)\n",
    "    \n",
    "    # Set output path\n",
    "    if debug == True:\n",
    "        mp4_path = os.path.join(A5_savepath, 'task1_input.mp4')\n",
    "    else:\n",
    "        mp4_path = 'task1_input.mp4'\n",
    "    \n",
    "    # Try different codecs for better compatibility\n",
    "    codecs_to_try = [\n",
    "        ('avc1', 'H.264 (avc1)'),\n",
    "        ('H264', 'H.264 (H264)'),\n",
    "        ('X264', 'H.264 (X264)'),\n",
    "        ('mp4v', 'MPEG-4'),\n",
    "        ('XVID', 'XVID')\n",
    "    ]\n",
    "    \n",
    "    writer = None\n",
    "    codec_used = None\n",
    "    \n",
    "    for codec, codec_name in codecs_to_try:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*codec)\n",
    "        test_writer = cv2.VideoWriter(mp4_path, fourcc, fps, frame_size)\n",
    "        if test_writer.isOpened():\n",
    "            writer = test_writer\n",
    "            codec_used = codec_name\n",
    "            print(f\"Using codec: {codec_name}\")\n",
    "            break\n",
    "        else:\n",
    "            test_writer.release()\n",
    "    \n",
    "    if writer is None or not writer.isOpened():\n",
    "        print(\"ERROR: Could not open VideoWriter with any codec!\")\n",
    "    else:\n",
    "        # Write all frames to video\n",
    "        frame_count = 0\n",
    "        for img_file in image_files:\n",
    "            img_path = Path(images1_path, img_file)\n",
    "            img_bgr = cv2.imread(str(img_path))\n",
    "            \n",
    "            if img_bgr is not None:\n",
    "                # Ensure frame is correct size\n",
    "                if img_bgr.shape[:2] != (H, W):\n",
    "                    img_bgr = cv2.resize(img_bgr, (W, H))\n",
    "                \n",
    "                # cv2.VideoWriter expects BGR format\n",
    "                writer.write(img_bgr)\n",
    "                frame_count += 1\n",
    "            else:\n",
    "                print(f\"Warning: Could not read {img_file}\")\n",
    "        \n",
    "        # IMPORTANT: Properly release the writer\n",
    "        writer.release()\n",
    "        \n",
    "        # Verify file was created\n",
    "        if os.path.exists(mp4_path):\n",
    "            file_size_mb = os.path.getsize(mp4_path) / (1024 * 1024)\n",
    "            print(f\"\\n✓ Part 1 Complete: Video created successfully!\")\n",
    "            print(f\"  Output: {mp4_path}\")\n",
    "            print(f\"  Codec: {codec_used}\")\n",
    "            print(f\"  Frames written: {frame_count}/{len(image_files)}\")\n",
    "            print(f\"  FPS: {fps}\")\n",
    "            print(f\"  Duration: {frame_count/fps:.2f} seconds\")\n",
    "            print(f\"  File size: {file_size_mb:.2f} MB\")\n",
    "        else:\n",
    "            print(\"ERROR: Video file was not created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PART 2: Model Implementation (40 points) =====\n",
    "# Install required packages for YOLOv8 + DeepSORT\n",
    "\n",
    "# Install ultralytics (YOLOv8)\n",
    "!pip install ultralytics\n",
    "\n",
    "# Install deep-sort-realtime\n",
    "!pip install deep-sort-realtime\n",
    "\n",
    "print(\"✓ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d431ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PART 2: YOLOv8 + DeepSORT Tracking Implementation =====\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Initialize YOLOv8 model\n",
    "print(\"Loading YOLOv8 model...\")\n",
    "model = YOLO('yolov8n.pt')  # Using nano model for speed, can use yolov8s.pt, yolov8m.pt for better accuracy\n",
    "print(\"✓ YOLOv8 model loaded\")\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "print(\"Initializing DeepSORT tracker...\")\n",
    "tracker = DeepSort(\n",
    "    max_age=30,           # Maximum frames to keep track alive without detections\n",
    "    n_init=3,             # Number of consecutive frames for track confirmation\n",
    "    max_iou_distance=0.7, # Maximum IOU distance for matching\n",
    "    embedder=\"mobilenet\", # Feature extractor\n",
    "    half=True,            # Use FP16 for speed\n",
    "    embedder_gpu=torch.cuda.is_available()\n",
    ")\n",
    "print(\"✓ DeepSORT tracker initialized\")\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(mp4_path)\n",
    "\n",
    "# Get video properties\n",
    "fps_input = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"\\nInput Video Properties:\")\n",
    "print(f\"  Resolution: {width}x{height}\")\n",
    "print(f\"  FPS: {fps_input}\")\n",
    "print(f\"  Total frames: {total_frames}\")\n",
    "\n",
    "# Set up output video writer\n",
    "if debug == True:\n",
    "    output_path = os.path.join(A5_savepath, 'task1.mp4')\n",
    "    tracking_results_path = os.path.join(A5_savepath, 'task1_tracking.txt')\n",
    "else:\n",
    "    output_path = 'task1.mp4'\n",
    "    tracking_results_path = 'task1_tracking.txt'\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps_input, (width, height))\n",
    "\n",
    "# Open file to save tracking results\n",
    "tracking_file = open(tracking_results_path, 'w')\n",
    "\n",
    "# Process video frame by frame\n",
    "frame_idx = 0\n",
    "tracking_data = []\n",
    "\n",
    "print(\"\\nProcessing video with YOLOv8 + DeepSORT...\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_idx += 1\n",
    "    \n",
    "    # Run YOLOv8 detection (class 0 is 'person' in COCO dataset)\n",
    "    results = model(frame, classes=[0], verbose=False)  # Only detect persons\n",
    "    \n",
    "    # Extract detections for DeepSORT\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            # Get bounding box coordinates\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = box.conf[0].cpu().numpy()\n",
    "            \n",
    "            # Convert to [left, top, width, height] format for DeepSORT\n",
    "            bbox = [x1, y1, x2 - x1, y2 - y1]\n",
    "            \n",
    "            # DeepSORT expects: ([left, top, width, height], confidence, class_name)\n",
    "            detections.append((bbox, conf, 'person'))\n",
    "    \n",
    "    # Update tracker with detections\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "    \n",
    "    # Draw bounding boxes and IDs on frame\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        \n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()  # Get [left, top, right, bottom]\n",
    "        \n",
    "        x1, y1, x2, y2 = map(int, ltrb)\n",
    "        bb_left = x1\n",
    "        bb_top = y1\n",
    "        bb_width = x2 - x1\n",
    "        bb_height = y2 - y1\n",
    "        \n",
    "        # Save tracking result: <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>\n",
    "        tracking_file.write(f\"{frame_idx},{track_id},{bb_left},{bb_top},{bb_width},{bb_height}\\n\")\n",
    "        tracking_data.append([frame_idx, track_id, bb_left, bb_top, bb_width, bb_height])\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw tracking ID\n",
    "        label = f'ID: {track_id}'\n",
    "        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "        label_y = max(y1 - 10, label_size[1] + 10)\n",
    "        \n",
    "        # Draw background for text\n",
    "        cv2.rectangle(frame, (x1, label_y - label_size[1] - 10), \n",
    "                     (x1 + label_size[0], label_y), (0, 255, 0), -1)\n",
    "        \n",
    "        # Draw text\n",
    "        cv2.putText(frame, label, (x1, label_y - 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "    \n",
    "    # Add frame number to video\n",
    "    cv2.putText(frame, f'Frame: {frame_idx}/{total_frames}', (10, 30), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    # Write frame to output video\n",
    "    out.write(frame)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if frame_idx % 50 == 0:\n",
    "        print(f\"  Processed frame {frame_idx}/{total_frames} ({frame_idx/total_frames*100:.1f}%)\")\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "tracking_file.close()\n",
    "\n",
    "print(f\"Output video: {output_path}\")\n",
    "print(f\"Tracking results: {tracking_results_path}\")\n",
    "print(f\"Total frames processed: {frame_idx}\")\n",
    "print(f\"Total tracked detections: {len(tracking_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tlwqnjxorvm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Display Sample Tracked Frame and Statistics =====\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load tracking results\n",
    "tracking_df = pd.DataFrame(tracking_data, columns=['frame', 'id', 'bb_left', 'bb_top', 'bb_width', 'bb_height'])\n",
    "\n",
    "print(\"Tracking Statistics:\")\n",
    "print(f\"  Total detections: {len(tracking_df)}\")\n",
    "print(f\"  Unique track IDs: {tracking_df['id'].nunique()}\")\n",
    "print(f\"  Frames with detections: {tracking_df['frame'].nunique()}\")\n",
    "print(f\"  Average detections per frame: {len(tracking_df) / tracking_df['frame'].nunique():.2f}\")\n",
    "\n",
    "# Show distribution of track IDs\n",
    "print(\"\\nTrack ID distribution (top 10):\")\n",
    "print(tracking_df['id'].value_counts().head(10))\n",
    "\n",
    "# Display a sample tracked frame\n",
    "sample_frame_num = total_frames // 2  # Middle frame\n",
    "cap_sample = cv2.VideoCapture(output_path)\n",
    "cap_sample.set(cv2.CAP_PROP_POS_FRAMES, sample_frame_num)\n",
    "ret, sample_frame = cap_sample.read()\n",
    "cap_sample.release()\n",
    "\n",
    "if ret:\n",
    "    # Convert BGR to RGB for matplotlib\n",
    "    sample_frame_rgb = cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(sample_frame_rgb)\n",
    "    plt.title(f'Sample Tracked Frame (Frame {sample_frame_num})')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd2c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PART 3: Model Evaluation (40 points) =====\n",
    "# Calculate MOTA metric using ground truth and predictions\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# box format: [x, y, width, height]\n",
    "def compute_iou(box1, box2):\n",
    "    x1_min = box1[0]\n",
    "    y1_min = box1[1]\n",
    "    x1_max = box1[0] + box1[2]\n",
    "    y1_max = box1[1] + box1[3]\n",
    "\n",
    "    x2_min = box2[0]\n",
    "    y2_min = box2[1]\n",
    "    x2_max = box2[0] + box2[2]\n",
    "    y2_max = box2[1] + box2[3]\n",
    "    \n",
    "    overlap_x_min = max(x1_min, x2_min)\n",
    "    overlap_y_min = max(y1_min, y2_min)\n",
    "    overlap_x_max = min(x1_max, x2_max)\n",
    "    overlap_y_max = min(y1_max, y2_max)\n",
    "\n",
    "    overlap_width = overlap_x_max - overlap_x_min\n",
    "    overlap_height = overlap_y_max - overlap_y_min\n",
    "\n",
    "    if overlap_width <= 0 or overlap_height <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection_area = overlap_width * overlap_height\n",
    "    union_area = box1[2] * box1[3] + box2[2] * box2[3] - intersection_area\n",
    "\n",
    "    return intersection_area / union_area\n",
    "\n",
    "\n",
    "# gt/pred boxes format: [[x, y, w, h], [x, y, w, h], ...]\n",
    "# returns IoU matrix (rows=GT, cols=Pred)\n",
    "def compute_iou_one_frame(gt_boxes, pred_boxes):\n",
    "    matrix = np.zeros((len(gt_boxes), len(pred_boxes)))\n",
    "    for i, gt_box in enumerate(gt_boxes):\n",
    "        for j, pred_box in enumerate(pred_boxes):\n",
    "            matrix[i, j] = compute_iou(gt_box, pred_box)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Match GT boxes to predicted boxes using Hungarian algorithm\n",
    "# Returns: matches, unmatched_gt, unmatched_pred\n",
    "def match_boxes(gt_boxes, pred_boxes, iou_threshold=0.5):\n",
    "    # Handle edge cases\n",
    "    if len(gt_boxes) == 0 or len(pred_boxes) == 0:\n",
    "        return [], list(range(len(gt_boxes))), list(range(len(pred_boxes)))\n",
    "    \n",
    "    # Compute IoU matrix\n",
    "    iou_matrix = compute_iou_one_frame(gt_boxes, pred_boxes)\n",
    "    \n",
    "    # Hungarian algorithm (minimizes cost, so negate IoU)\n",
    "    cost_matrix = -iou_matrix\n",
    "    gt_indices, pred_indices = linear_sum_assignment(cost_matrix)\n",
    "    \n",
    "    # Filter matches by IoU threshold\n",
    "    matches = []\n",
    "    matched_gt = set()\n",
    "    matched_pred = set()\n",
    "    \n",
    "    for gt_idx, pred_idx in zip(gt_indices, pred_indices):\n",
    "        iou = iou_matrix[gt_idx, pred_idx]\n",
    "        if iou >= iou_threshold:\n",
    "            matches.append((gt_idx, pred_idx, iou))\n",
    "            matched_gt.add(gt_idx)\n",
    "            matched_pred.add(pred_idx)\n",
    "    \n",
    "    # Find unmatched boxes\n",
    "    unmatched_gt = [i for i in range(len(gt_boxes)) if i not in matched_gt]\n",
    "    unmatched_pred = [i for i in range(len(pred_boxes)) if i not in matched_pred]\n",
    "    \n",
    "    return matches, unmatched_gt, unmatched_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0m63vmye81y",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PART 3: Calculate MOTA =====\n",
    "\n",
    "print(\"We have the following data:\")\n",
    "\n",
    "# Load ground truth data\n",
    "gt_data = ground_truth_task1  # Already loaded in Part 1\n",
    "print(f\"  Ground truth: {len(gt_data)} detections\")\n",
    "\n",
    "# Load prediction data\n",
    "if debug == True:\n",
    "    pred_path = os.path.join(A5_savepath, 'task1_tracking.txt')\n",
    "else:\n",
    "    pred_path = 'task1_tracking.txt'\n",
    "\n",
    "pred_data = np.loadtxt(pred_path, delimiter=',')\n",
    "print(f\"  Predictions: {len(pred_data)} detections\")\n",
    "\n",
    "# Get frame range\n",
    "max_frame = int(max(gt_data[:, 0].max(), pred_data[:, 0].max()))\n",
    "min_frame = 1\n",
    "print(f\"  Frame range: {min_frame} to {max_frame}\")\n",
    "\n",
    "# Initialize counters\n",
    "total_FN = 0\n",
    "total_FP = 0\n",
    "total_IDSW = 0\n",
    "total_GT = 0\n",
    "\n",
    "# Track GT object IDs across frames for identity switch detection\n",
    "# Maps: gt_id -> pred_id from previous frame\n",
    "gt_to_pred_id = {}\n",
    "\n",
    "print(\"\\nMOTA Metrics processing\")\n",
    "\n",
    "# Process each frame\n",
    "for frame_num in range(min_frame, max_frame + 1):\n",
    "    # Get boxes for this frame\n",
    "    gt_frame = gt_data[gt_data[:, 0] == frame_num]\n",
    "    pred_frame = pred_data[pred_data[:, 0] == frame_num]\n",
    "    \n",
    "    # Extract box coordinates [x, y, w, h]\n",
    "    gt_boxes = gt_frame[:, 2:6]  # columns: bb_left, bb_top, bb_width, bb_height\n",
    "    pred_boxes = pred_frame[:, 2:6]\n",
    "    \n",
    "    # Extract IDs\n",
    "    gt_ids = gt_frame[:, 1] if len(gt_frame) > 0 else []\n",
    "    pred_ids = pred_frame[:, 1] if len(pred_frame) > 0 else []\n",
    "    \n",
    "    # Match boxes using Hungarian algorithm\n",
    "    matches, unmatched_gt, unmatched_pred = match_boxes(gt_boxes, pred_boxes, iou_threshold=0.5)\n",
    "    \n",
    "    # Count false negatives and false positives\n",
    "    FN = len(unmatched_gt)\n",
    "    FP = len(unmatched_pred)\n",
    "    GT = len(gt_boxes)\n",
    "    \n",
    "    total_FN += FN\n",
    "    total_FP += FP\n",
    "    total_GT += GT\n",
    "    \n",
    "    # Count identity switches\n",
    "    IDSW = 0\n",
    "    for gt_idx, pred_idx, iou in matches:\n",
    "        gt_id = gt_ids[gt_idx]\n",
    "        pred_id = pred_ids[pred_idx]\n",
    "        \n",
    "        # Check if this GT object was tracked before\n",
    "        if gt_id in gt_to_pred_id:\n",
    "            # If it was matched to a different prediction ID, it's an identity switch\n",
    "            if gt_to_pred_id[gt_id] != pred_id:\n",
    "                IDSW += 1\n",
    "        \n",
    "        # Update tracking\n",
    "        gt_to_pred_id[gt_id] = pred_id\n",
    "    \n",
    "    total_IDSW += IDSW\n",
    "    \n",
    "    # Progress indicator\n",
    "    if frame_num % 50 == 0:\n",
    "        print(f\"  Processed frame {frame_num}/{max_frame}\")\n",
    "\n",
    "# Calculate MOTA\n",
    "if total_GT > 0:\n",
    "    MOTA = 1 - (total_FN + total_FP + total_IDSW) / total_GT\n",
    "else:\n",
    "    MOTA = 0.0\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MOTA EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"False Negatives (FN):    {total_FN}\")\n",
    "print(f\"False Positives (FP):    {total_FP}\")\n",
    "print(f\"Identity Switches (IDSW): {total_IDSW}\")\n",
    "print(f\"Ground Truth Objects (GT): {total_GT}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"MOTA Score: {MOTA:.4f} ({MOTA*100:.2f}%)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nBreakdown:\")\n",
    "print(f\"  Miss Rate (FN/GT):     {total_FN/total_GT:.2%}\" if total_GT > 0 else \"  Miss Rate: N/A\")\n",
    "print(f\"  False Alarm Rate:      {total_FP/total_GT:.2%}\" if total_GT > 0 else \"  False Alarm Rate: N/A\")\n",
    "print(f\"  ID Switch Rate:        {total_IDSW/total_GT:.2%}\" if total_GT > 0 else \"  ID Switch Rate: N/A\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9im38uzr8ih",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Visualize MOTA Results =====\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Error breakdown\n",
    "error_types = ['False\\nNegatives', 'False\\nPositives', 'Identity\\nSwitches']\n",
    "error_counts = [total_FN, total_FP, total_IDSW]\n",
    "colors = ['#ff6b6b', '#feca57', '#48dbfb']\n",
    "\n",
    "axes[0].bar(error_types, error_counts, color=colors)\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Error Breakdown')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, count in enumerate(error_counts):\n",
    "    axes[0].text(i, count + max(error_counts)*0.02, str(count), \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: MOTA components as percentages\n",
    "if total_GT > 0:\n",
    "    percentages = [\n",
    "        total_FN / total_GT * 100,\n",
    "        total_FP / total_GT * 100,\n",
    "        total_IDSW / total_GT * 100\n",
    "    ]\n",
    "    \n",
    "    axes[1].bar(error_types, percentages, color=colors)\n",
    "    axes[1].set_ylabel('Percentage of GT (%)')\n",
    "    axes[1].set_title('Error Rates')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, pct in enumerate(percentages):\n",
    "        axes[1].text(i, pct + max(percentages)*0.02, f'{pct:.1f}%', \n",
    "                    ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nSummary Table:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<25} {'Count':<15} {'% of GT':<15}\")\n",
    "print(\"-\" * 60)\n",
    "if total_GT > 0:\n",
    "    print(f\"{'False Negatives (FN)':<25} {total_FN:<15} {total_FN/total_GT*100:>6.2f}%\")\n",
    "    print(f\"{'False Positives (FP)':<25} {total_FP:<15} {total_FP/total_GT*100:>6.2f}%\")\n",
    "    print(f\"{'Identity Switches (IDSW)':<25} {total_IDSW:<15} {total_IDSW/total_GT*100:>6.2f}%\")\n",
    "    print(f\"{'Ground Truth (GT)':<25} {total_GT:<15} {'100.00%':>11}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'MOTA Score':<25} {MOTA:<15.4f} {MOTA*100:>6.2f}%\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ex1vuw2r4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PART 2: YOLOv8 + ByteTrack Tracking Implementation =====\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize YOLOv8 model (medium for better accuracy at distance)\n",
    "print(\"Loading YOLOv8 model...\")\n",
    "model = YOLO('yolov8m.pt')  # Can also try yolov8l.pt for even better accuracy\n",
    "print(\"✓ YOLOv8 model loaded\")\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(mp4_path)\n",
    "\n",
    "# Get video properties\n",
    "fps_input = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"\\nInput Video Properties:\")\n",
    "print(f\"  Resolution: {width}x{height}\")\n",
    "print(f\"  FPS: {fps_input}\")\n",
    "print(f\"  Total frames: {total_frames}\")\n",
    "\n",
    "# Set up output video writer\n",
    "if debug == True:\n",
    "    output_path = os.path.join(A5_savepath, 'task2.mp4')\n",
    "    tracking_results_path = os.path.join(A5_savepath, 'task2_tracking.txt')\n",
    "else:\n",
    "    output_path = 'task2.mp4'\n",
    "    tracking_results_path = 'task2_tracking.txt'\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps_input, (width, height))\n",
    "\n",
    "# Open file to save tracking results\n",
    "tracking_file = open(tracking_results_path, 'w')\n",
    "\n",
    "# Process video frame by frame\n",
    "frame_idx = 0\n",
    "tracking_data = []\n",
    "\n",
    "print(\"\\nProcessing video with YOLOv8 + ByteTrack...\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_idx += 1\n",
    "    \n",
    "    # Run YOLOv8 tracking (ByteTrack built-in)\n",
    "    results = model.track(\n",
    "        source=frame,\n",
    "        classes=[0],              # Only track people (class 0)\n",
    "        conf=0.35,                # Lower confidence for better recall\n",
    "        iou=0.7,                  # NMS IoU threshold\n",
    "        tracker='bytetrack.yaml', # Use ByteTrack\n",
    "        persist=True,             # Persist tracks across frames\n",
    "        verbose=False,\n",
    "        imgsz=640                 # Can increase to 1280 for small objects\n",
    "    )\n",
    "    \n",
    "    # Extract tracking results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        if boxes is not None and boxes.id is not None:\n",
    "            for box_coords, track_id in zip(boxes.xyxy, boxes.id):\n",
    "                x1, y1, x2, y2 = box_coords.cpu().numpy()\n",
    "                track_id = int(track_id.cpu().numpy())\n",
    "                \n",
    "                bb_left = int(x1)\n",
    "                bb_top = int(y1)\n",
    "                bb_width = int(x2 - x1)\n",
    "                bb_height = int(y2 - y1)\n",
    "                \n",
    "                # Save tracking result\n",
    "                tracking_file.write(f\"{frame_idx},{track_id},{bb_left},{bb_top},{bb_width},{bb_height}\\n\")\n",
    "                tracking_data.append([frame_idx, track_id, bb_left, bb_top, bb_width, bb_height])\n",
    "                \n",
    "                # Draw on frame\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                \n",
    "                label = f'ID: {track_id}'\n",
    "                label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                label_y = max(int(y1) - 10, label_size[1] + 10)\n",
    "                \n",
    "                cv2.rectangle(frame, (int(x1), label_y - label_size[1] - 10), \n",
    "                             (int(x1) + label_size[0], label_y), (0, 255, 0), -1)\n",
    "                cv2.putText(frame, label, (int(x1), label_y - 5), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "    \n",
    "    # Add frame number\n",
    "    cv2.putText(frame, f'Frame: {frame_idx}/{total_frames}', (10, 30), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    out.write(frame)\n",
    "    \n",
    "    if frame_idx % 50 == 0:\n",
    "        print(f\"  Processed frame {frame_idx}/{total_frames} ({frame_idx/total_frames*100:.1f}%)\")\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "tracking_file.close()\n",
    "\n",
    "print(f\"\\n✓ Tracking complete!\")\n",
    "print(f\"Output video: {output_path}\")\n",
    "print(f\"Tracking results: {tracking_results_path}\")\n",
    "print(f\"Total frames processed: {frame_idx}\")\n",
    "print(f\"Total tracked detections: {len(tracking_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
