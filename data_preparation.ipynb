{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1def7355",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33m/content/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     16\u001b[39m debug = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m     19\u001b[39m drive.mount(path+\u001b[33m'\u001b[39m\u001b[33m/drive/\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug == \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "import sklearn\n",
    "import skimage\n",
    "from skimage import __version__ as skimage_version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "\n",
    "path = \"/content/\"\n",
    "debug = True\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(path+'/drive/')\n",
    "\n",
    "if debug == True:\n",
    "    A5_savepath = path+'/drive/MyDrive/03 McGill EE Semester 5 (Fall 2025)/ECSE 415 (Clark)/A5/'\n",
    "\n",
    "print(f\"Python version:           {os.sys.version.split()[0]}\")\n",
    "print(f\"OpenCV version:           {cv2.__version__}\")\n",
    "print(f\"NumPy version:            {np.__version__}\")\n",
    "print(f\"Matplotlib version:       {matplotlib.__version__}\")\n",
    "print(f\"PyTorch version:          {torch.__version__}\")\n",
    "print(f\"scikit-image version:     {skimage_version}\")\n",
    "print(f\"scikit-learn version:     {sklearn_version}\")\n",
    "print(\"Path: \" + path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7cde26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug == False:\n",
    "    os.chdir(path)\n",
    "    !pip install kaggle\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp /content/drive/MyDrive/Kaggle_API/kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    !kaggle competitions download -c ecse-415-video-analysis\n",
    "    !unzip -q ecse-415-video-analysis.zip -d ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010737a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PART 1: Data Preparation (10 points) =====\n",
    "# Convert Task1 images to video at 14 FPS and save as task1_input.mp4\n",
    "\n",
    "task1_dir = Path(path, \"Tracking\", \"Task1\")\n",
    "csv1_path = Path(task1_dir, \"gt\", \"gt.txt\")\n",
    "ground_truth_task1 = np.loadtxt(csv1_path, delimiter=\",\")\n",
    "\n",
    "images1_path = Path(task1_dir, \"images\")\n",
    "\n",
    "# Get all image files and sort them by filename to ensure correct order\n",
    "image_files = sorted([f for f in os.listdir(images1_path) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "\n",
    "print(f\"Found {len(image_files)} images in {images1_path}\")\n",
    "\n",
    "if len(image_files) == 0:\n",
    "    print(\"ERROR: No images found!\")\n",
    "else:\n",
    "    # Read first image to get dimensions\n",
    "    first_img_path = Path(images1_path, image_files[0])\n",
    "    first_frame = cv2.imread(str(first_img_path))\n",
    "    H, W, _ = first_frame.shape\n",
    "    \n",
    "    print(f\"Video dimensions: {W}x{H}\")\n",
    "    \n",
    "    # Set up VideoWriter\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
    "    fps = 14.0  # Required FPS for Part 1\n",
    "    frame_size = (W, H)\n",
    "    \n",
    "    # Set output path\n",
    "    if debug == True:\n",
    "        mp4_path = os.path.join(A5_savepath, 'task1_input.mp4')\n",
    "    else:\n",
    "        mp4_path = 'task1_input.mp4'\n",
    "    \n",
    "    writer = cv2.VideoWriter(mp4_path, fourcc, fps, frame_size)\n",
    "    \n",
    "    # Check if writer opened successfully\n",
    "    if not writer.isOpened():\n",
    "        print(\"ERROR: Could not open VideoWriter!\")\n",
    "    else:\n",
    "        # Write all frames to video\n",
    "        for img_file in image_files:\n",
    "            img_path = Path(images1_path, img_file)\n",
    "            img_bgr = cv2.imread(str(img_path))\n",
    "            \n",
    "            if img_bgr is not None:\n",
    "                # cv2.VideoWriter expects BGR format\n",
    "                writer.write(img_bgr)\n",
    "            else:\n",
    "                print(f\"Warning: Could not read {img_file}\")\n",
    "        \n",
    "        writer.release()\n",
    "        print(f\"✓ Part 1 Complete: Wrote video to {mp4_path}\")\n",
    "        print(f\"  Total frames: {len(image_files)}\")\n",
    "        print(f\"  FPS: {fps}\")\n",
    "        print(f\"  Duration: {len(image_files)/fps:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee1b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PART 2: Model Implementation (40 points) =====\n",
    "# Install required packages for YOLOv8 + DeepSORT\n",
    "\n",
    "# Install ultralytics (YOLOv8)\n",
    "!pip install ultralytics\n",
    "\n",
    "# Install deep-sort-realtime\n",
    "!pip install deep-sort-realtime\n",
    "\n",
    "print(\"✓ Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d431ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PART 2: YOLOv8 + DeepSORT Tracking Implementation =====\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# Initialize YOLOv8 model\n",
    "print(\"Loading YOLOv8 model...\")\n",
    "model = YOLO('yolov8n.pt')  # Using nano model for speed, can use yolov8s.pt, yolov8m.pt for better accuracy\n",
    "print(\"✓ YOLOv8 model loaded\")\n",
    "\n",
    "# Initialize DeepSORT tracker\n",
    "print(\"Initializing DeepSORT tracker...\")\n",
    "tracker = DeepSort(\n",
    "    max_age=30,           # Maximum frames to keep track alive without detections\n",
    "    n_init=3,             # Number of consecutive frames for track confirmation\n",
    "    max_iou_distance=0.7, # Maximum IOU distance for matching\n",
    "    embedder=\"mobilenet\", # Feature extractor\n",
    "    half=True,            # Use FP16 for speed\n",
    "    embedder_gpu=torch.cuda.is_available()\n",
    ")\n",
    "print(\"✓ DeepSORT tracker initialized\")\n",
    "\n",
    "# Open input video\n",
    "cap = cv2.VideoCapture(mp4_path)\n",
    "\n",
    "# Get video properties\n",
    "fps_input = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "print(f\"\\nInput Video Properties:\")\n",
    "print(f\"  Resolution: {width}x{height}\")\n",
    "print(f\"  FPS: {fps_input}\")\n",
    "print(f\"  Total frames: {total_frames}\")\n",
    "\n",
    "# Set up output video writer\n",
    "if debug == True:\n",
    "    output_path = os.path.join(A5_savepath, 'task1.mp4')\n",
    "    tracking_results_path = os.path.join(A5_savepath, 'task1_tracking.txt')\n",
    "else:\n",
    "    output_path = 'task1.mp4'\n",
    "    tracking_results_path = 'task1_tracking.txt'\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps_input, (width, height))\n",
    "\n",
    "# Open file to save tracking results\n",
    "tracking_file = open(tracking_results_path, 'w')\n",
    "\n",
    "# Process video frame by frame\n",
    "frame_idx = 0\n",
    "tracking_data = []\n",
    "\n",
    "print(\"\\nProcessing video with YOLOv8 + DeepSORT...\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_idx += 1\n",
    "    \n",
    "    # Run YOLOv8 detection (class 0 is 'person' in COCO dataset)\n",
    "    results = model(frame, classes=[0], verbose=False)  # Only detect persons\n",
    "    \n",
    "    # Extract detections for DeepSORT\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            # Get bounding box coordinates\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            conf = box.conf[0].cpu().numpy()\n",
    "            \n",
    "            # Convert to [left, top, width, height] format for DeepSORT\n",
    "            bbox = [x1, y1, x2 - x1, y2 - y1]\n",
    "            \n",
    "            # DeepSORT expects: ([left, top, width, height], confidence, class_name)\n",
    "            detections.append((bbox, conf, 'person'))\n",
    "    \n",
    "    # Update tracker with detections\n",
    "    tracks = tracker.update_tracks(detections, frame=frame)\n",
    "    \n",
    "    # Draw bounding boxes and IDs on frame\n",
    "    for track in tracks:\n",
    "        if not track.is_confirmed():\n",
    "            continue\n",
    "        \n",
    "        track_id = track.track_id\n",
    "        ltrb = track.to_ltrb()  # Get [left, top, right, bottom]\n",
    "        \n",
    "        x1, y1, x2, y2 = map(int, ltrb)\n",
    "        bb_left = x1\n",
    "        bb_top = y1\n",
    "        bb_width = x2 - x1\n",
    "        bb_height = y2 - y1\n",
    "        \n",
    "        # Save tracking result: <frame>, <id>, <bb_left>, <bb_top>, <bb_width>, <bb_height>\n",
    "        tracking_file.write(f\"{frame_idx},{track_id},{bb_left},{bb_top},{bb_width},{bb_height}\\n\")\n",
    "        tracking_data.append([frame_idx, track_id, bb_left, bb_top, bb_width, bb_height])\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw tracking ID\n",
    "        label = f'ID: {track_id}'\n",
    "        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "        label_y = max(y1 - 10, label_size[1] + 10)\n",
    "        \n",
    "        # Draw background for text\n",
    "        cv2.rectangle(frame, (x1, label_y - label_size[1] - 10), \n",
    "                     (x1 + label_size[0], label_y), (0, 255, 0), -1)\n",
    "        \n",
    "        # Draw text\n",
    "        cv2.putText(frame, label, (x1, label_y - 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n",
    "    \n",
    "    # Add frame number to video\n",
    "    cv2.putText(frame, f'Frame: {frame_idx}/{total_frames}', (10, 30), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    # Write frame to output video\n",
    "    out.write(frame)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if frame_idx % 50 == 0:\n",
    "        print(f\"  Processed frame {frame_idx}/{total_frames} ({frame_idx/total_frames*100:.1f}%)\")\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "out.release()\n",
    "tracking_file.close()\n",
    "\n",
    "print(f\"\\n✓ Part 2 Complete: Object tracking finished!\")\n",
    "print(f\"  Output video: {output_path}\")\n",
    "print(f\"  Tracking results: {tracking_results_path}\")\n",
    "print(f\"  Total frames processed: {frame_idx}\")\n",
    "print(f\"  Total tracked detections: {len(tracking_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tlwqnjxorvm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Display Sample Tracked Frame and Statistics =====\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load tracking results\n",
    "tracking_df = pd.DataFrame(tracking_data, columns=['frame', 'id', 'bb_left', 'bb_top', 'bb_width', 'bb_height'])\n",
    "\n",
    "print(\"Tracking Statistics:\")\n",
    "print(f\"  Total detections: {len(tracking_df)}\")\n",
    "print(f\"  Unique track IDs: {tracking_df['id'].nunique()}\")\n",
    "print(f\"  Frames with detections: {tracking_df['frame'].nunique()}\")\n",
    "print(f\"  Average detections per frame: {len(tracking_df) / tracking_df['frame'].nunique():.2f}\")\n",
    "\n",
    "# Show distribution of track IDs\n",
    "print(\"\\nTrack ID distribution (top 10):\")\n",
    "print(tracking_df['id'].value_counts().head(10))\n",
    "\n",
    "# Display a sample tracked frame\n",
    "sample_frame_num = total_frames // 2  # Middle frame\n",
    "cap_sample = cv2.VideoCapture(output_path)\n",
    "cap_sample.set(cv2.CAP_PROP_POS_FRAMES, sample_frame_num)\n",
    "ret, sample_frame = cap_sample.read()\n",
    "cap_sample.release()\n",
    "\n",
    "if ret:\n",
    "    # Convert BGR to RGB for matplotlib\n",
    "    sample_frame_rgb = cv2.cvtColor(sample_frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(sample_frame_rgb)\n",
    "    plt.title(f'Sample Tracked Frame (Frame {sample_frame_num})')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "print(\"\\n✓ All outputs generated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
